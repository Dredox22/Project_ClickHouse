# Betting Data Aggregation in ClickHouse

Этот проект демонстрирует решение для инкрементальной агрегации данных ставок клиентов из потоковой таблицы `raw_bets` в ClickHouse. Данные содержат ставки с несколькими позициями, где сумма (`amount`) дублируется внутри одной ставки (`order`), и требуется подсчитать сумму уникальных ставок по пользователям (`user`).

## Описание задачи

- **Входные данные**: Таблица `raw_bets` с полями `user`, `order`, `order_position`, `amount`, `bet_date`.
  - У одного пользователя может быть много ставок.
  - У каждой ставки может быть несколько позиций.
  - Поле `amount` одинаково для всех позиций внутри одной ставки.
- **Цель**: Получить таблицу `user_totals` с агрегатами по пользователям:

  ```
  user | amount
  1    | 150.00
  2    | 75.00
  ```

  где `amount` — сумма уникальных ставок, а не позиций.
- **Условия**:
  - Данные поступают в потоке, объем — миллионы строк.
  - Подсчеты должны быть инкрементальными.
  - Нельзя использовать `OPTIMIZE` (чтобы не перегружать базу).
  - Нельзя использовать `FINAL` в материализованных представлениях (MV).

## Структура проекта

- `generate_bets.py`: Скрипт для генерации 1 млн строк тестовых данных в файл `raw_bets_1m.csv`.
- `setup.sql`: SQL-скрипт для создания таблиц и материализованных представлений в ClickHouse.
- `raw_bets_1m.csv`: Сгенерированный файл с данными (создается после запуска `generate_bets.py`).
- `README.md`: Этот файл с документацией.

## Требования

- **Python 3.8+**: Для запуска генератора данных.
- Установите зависимости: `pip install faker`
- **ClickHouse**: Установленный сервер (локально или в облаке).
  - Рекомендуемая версия: 23.8 или новее.
- **Дисковое пространство**: ~200 МБ для 1 млн строк в CSV.

## Установка и запуск

### 1. Генерация тестовых данных

1. Сохраните `generate_bets.py` в корневую папку проекта.
2. Установите зависимости:
   ```bash
   pip install faker
   ```
3. Запустите скрипт:
   ```bash
   python generate_bets.py
   ```
   Результат: файл `raw_bets_1m.csv` с 1 млн строк.

### 2. Настройка ClickHouse

1. Сохраните `setup.sql` в корневую папку проекта.
2. Подключитесь к ClickHouse и выполните скрипт:
   ```bash
   clickhouse-client < setup.sql
   ```
   Создаются:
   - Таблица `raw_bets` (ReplacingMergeTree).
   - Таблица `unique_orders` (MergeTree) и MV `mv_unique_orders`.
   - Таблица `user_totals` (SummingMergeTree) и MV `mv_user_totals`.

### 3. Импорт данных

Импортируйте сгенерированные данные в `raw_bets`:
```bash
clickhouse-client --query="INSERT INTO raw_bets FORMAT CSVWithNames" < raw_bets_1m.csv
```
Время выполнения: ~5-10 секунд для 1 млн строк на современном сервере.

### 4. Проверка результата

Посмотрите промежуточные данные:
```sql
SELECT * FROM unique_orders ORDER BY user, order LIMIT 10;
```
Проверьте итоговые агрегаты:
```sql
SELECT * FROM user_totals ORDER BY user LIMIT 10;
```

## Архитектура решения

- **raw_bets**:
  - Движок: ReplacingMergeTree.
  - Партиционирование: toYYYYMM(bet_date).
  - Используется для дедупликации на случай дублирующих вставок.
- **mv_unique_orders**:
  - Исключает дубли `amount` внутри одной ставки с помощью `argMin(amount, order_position)`.
  - Результат записывается в `unique_orders` (MergeTree).
- **mv_user_totals**:
  - Агрегирует сумму по пользователям.
  - Результат идет в `user_totals` (SummingMergeTree) для инкрементального суммирования.

## Пример данных

### Входные данные (raw_bets):

```
user,order,order_position,amount,bet_date
1,1,1,100.00,2023-05-12 14:23:11
1,1,2,100.00,2023-05-12 14:23:11
1,2,1,50.00,2021-08-19 09:15:32
2,3,1,75.00,2022-11-03 22:47:09
```

### Промежуточные данные (unique_orders):

```
user | order | amount | bet_date
1    | 1     | 100.00 | 2023-05-12 14:23:11
1    | 2     | 50.00  | 2021-08-19 09:15:32
2    | 3     | 75.00  | 2022-11-03 22:47:09
```

### Итоговые данные (user_totals):

```
user | amount
1    | 150.00
2    | 75.00
```

## Оптимизация

- **Партиционирование**: Используется `toYYYYMM(bet_date)` для эффективного управления данными за много лет.
- **Инкрементальность**: Новые данные обрабатываются MV автоматически без пересчета старых.
- **Масштабируемость**: Для больших объемов настройте `max_insert_threads`:
  ```bash
  clickhouse-client --max_insert_threads=8 --query="INSERT INTO raw_bets FORMAT CSVWithNames" < raw_bets_1m.csv
  ```

## Дополнительные заметки

- **Буферная таблица**: В `setup.sql` есть опциональная таблица `raw_bets_buffer` для смягчения нагрузки при потоковой вставке. Используйте, если данные приходят через Kafka или другой поток.
- **Расширение данных**: Измените `NUM_ROWS` в `generate_bets.py` для генерации 10 млн строк или больше.
- **Дата**: Поле `bet_date` добавлено для реалистичности и партиционирования. Уберите, если не нужно.